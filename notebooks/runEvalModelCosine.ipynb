{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import lossFunctions\n",
    "from lossFunctions import f1, focal_loss\n",
    "from cosine_decay import CosineDecayWithRestarts\n",
    "\n",
    "modelFile = '../working/ResNet50_COSINE_A.h5'\n",
    "submitFile = '../working/submitResNet50_COSINE_A.csv'\n",
    "loss_function = 'binary_crossentropy'\n",
    "# loss_function = focal_loss\n",
    "threshold = 0.35 # due to different cost of True Positive vs False Positive, this is the probability threshold to predict the class as 'yes'\n",
    "\n",
    "lr1 = 0.0001\n",
    "lr2 = 0.0006\n",
    "lr_decay = 0.9\n",
    "cycle_length= 1\n",
    "cosine_min = 0.0001\n",
    "cosine_max = 0.0006\n",
    "mult_factor=2\n",
    "epochs1 = cycle_length\n",
    "epochs2 = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras; \n",
    "from keras import backend as K\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from scipy.misc import imread\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e2b769631bd1b633b806ea9787a7c0670853a2f"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b0cad91f8f6cc0536f7ccd726e0c7c093b7d55d"
   },
   "outputs": [],
   "source": [
    "#SIZE = 299   #Inception\n",
    "SIZE = 224   #ResNet50\n",
    "BATCH_SIZE = 16\n",
    "SEED = 777\n",
    "SHAPE = (224, 224, 3)\n",
    "#SHAPE = (229, 229, 3)\n",
    "VAL_RATIO = 0.2 # 20 % as validation\n",
    "\n",
    "DIR = '../input'\n",
    "gamma = 2.0\n",
    "import keras\n",
    "epsilon = K.epsilon()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DateTime \" + time.strftime(\"%c\"))\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "\n",
    "reverse_train_labels = dict((v,k) for k,v in label_names.items())\n",
    "\n",
    "def getTargetNames(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    names = []\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        names.append(name)\n",
    "    return row\n",
    "\n",
    "def fill_targets(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        row.loc[name] = 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, shuffle = False, use_cache = False, augment = False,channels=3):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.augment = augment\n",
    "        self.channels = channels\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], shape[2]), dtype=np.float16)\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        paths = np.array([self.paths[k] for k in indexes])\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.load_image(path)\n",
    "\n",
    "        y = np.array([self.labels[k] for k in indexes])\n",
    "#        y = self.labels[indexes]\n",
    "                \n",
    "        if self.augment == True:\n",
    "            seq = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    "                    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "            X = np.concatenate((X, seq.augment_images(X), seq.augment_images(X), seq.augment_images(X)), 0)\n",
    "            y = np.concatenate((y, y, y, y), 0)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "    \n",
    "    def load4Channel(self, path):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "        np.array(image_red_ch), \n",
    "        np.array(image_yellow_ch),     \n",
    "        np.array(image_green_ch), \n",
    "        np.array(image_blue_ch)), -1)\n",
    "        image = cv2.resize(image, (self.shape[0], self.shape[1]))\n",
    "        im = np.divide(image, 255) \n",
    "        return im\n",
    "\n",
    "    def load3Channel(self, path):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "        np.array(image_red_ch), \n",
    "        np.array(image_green_ch), \n",
    "        np.array(image_blue_ch)), -1)\n",
    "        image = cv2.resize(image, (self.shape[0], self.shape[1]))\n",
    "        im = np.divide(image, 255)\n",
    "        return im\n",
    "\n",
    "    def load_image(self, path):\n",
    "        if self.channels == 3:\n",
    "            return self.load3Channel(path)\n",
    "        else:\n",
    "            return self.load4Channel(path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "223b4a1f80c42c7342bcab614a9c580e46daadc1"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Multiply, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv2D\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications import Xception\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.metrics import categorical_accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05ea6f11b8f54e9549de9af0ee79d45a8f44c070"
   },
   "outputs": [],
   "source": [
    "def create_ResNet50_model(input_shape, n_out):\n",
    "\n",
    "    base_model = ResNet50(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=Input(shape=(224,224,3)))\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "#    keras_models_dir = \"../input/keras inception v3 notop v0.5\"\n",
    "#    base_model.load_weights('%s/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5' % keras_models_dir)\n",
    "    model.add(Conv2D(32, kernel_size=(1,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "#    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(n_out))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aed6c9c01538b9a08859449527e2606fed8390a4"
   },
   "outputs": [],
   "source": [
    "# split data into train, valid\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)  \n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(keys)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "\n",
    "#print(lastIndex, lastTrainIndex, int(lastTrainIndex * 0.8))\n",
    "pathsTrain = paths[0:lastTrainIndex]\n",
    "labelsTrain = labels[0:lastTrainIndex]\n",
    "pathsVal = paths[lastTrainIndex:]\n",
    "labelsVal = labels[lastTrainIndex:]\n",
    "\n",
    "print(paths.shape, labels.shape)\n",
    "print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "\n",
    "train_generator = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, SHAPE, use_cache=False, augment = False, shuffle = False)\n",
    "validation_generator = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, SHAPE, use_cache=False, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f92bb8cad6207177bb88e06923b392a2719a2b03"
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = ModelCheckpoint(modelFile, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "#checkpoint = ModelCheckpoint('../working/ResNet50A.h5', monitor='val_loss', verbose=1, \n",
    "#                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                verbose=1, mode='auto', epsilon=0.0001)\n",
    "cosine_restart = CosineDecayWithRestarts(min_lr = cosine_min , max_lr = cosine_max, lr_decay=lr_decay, steps_per_epoch=len(train_generator),\n",
    "                                        cycle_length=cycle_length, mult_factor=mult_factor)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80a5bfd9f8b8920a1f28d62b4b85340d87d19b22",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "\n",
    "model = create_ResNet50_model(SHAPE, n_out=28)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#model.layers[1].trainable = True\n",
    "#model.layers[2].trainable = True\n",
    "#model.layers[3].trainable = True\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True\n",
    "model.layers[-4].trainable = True\n",
    "model.layers[-5].trainable = True\n",
    "model.layers[-6].trainable = True\n",
    "model.layers[-7].trainable = True\n",
    "model.layers[-8].trainable = True\n",
    "model.layers[-9].trainable = True\n",
    "model.layers[-10].trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93af21931e735dae9fd5856969658b7b9bbc4966"
   },
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n",
    "    pt = K.clip(pt, epsilon, 1-epsilon)\n",
    "    CE = -K.log(pt)\n",
    "    FL = K.pow(1-pt, gamma) * CE\n",
    "    loss = K.sum(FL, axis=1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1PR(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1), K.mean(p), K.mean(r)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7542b22181caae1ab9415f6f7cd0a4e44002f212",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "#    loss='binary_crossentropy', \n",
    "#    loss=focal_loss,\n",
    "    loss=loss_function,\n",
    "#    optimizer=Adam(1e-03),    \n",
    "    optimizer=Adam(1e-3),    \n",
    "    metrics=['acc', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ff395a7d35521a5d824cb8e72085de22f62765c"
   },
   "outputs": [],
   "source": [
    "epochs1 = cycle_length\n",
    "\n",
    "hist1 = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs1, \n",
    "    verbose=1,\n",
    "    callbacks=[cosine_restart])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9845fe5679456e105ae92d99d87d663ae8a4fd4"
   },
   "outputs": [],
   "source": [
    "#model.compile(\n",
    "#    loss='binary_crossentropy', \n",
    "#    optimizer=Adam(1e-03),\n",
    "#    metrics=['acc'])\n",
    "#models['model'].compile(loss=focal_loss,\n",
    "#                        optimizer=Adam(1e-03),\n",
    "#                        metrics=['categorical_accuracy', 'binary_accuracy','acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "417fd007ea558d5f002547326b5038f7ee0f74f0"
   },
   "outputs": [],
   "source": [
    "# train all layers\n",
    "train_generator = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, SHAPE, use_cache=False, augment = True, shuffle = False)\n",
    "validation_generator = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, SHAPE, use_cache=False, shuffle = False)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(\n",
    "#    loss='binary_crossentropy',\n",
    "     loss=loss_function, \n",
    "#     optimizer=Adam(lr=1e-4),\n",
    "     optimizer=Adam(1e-4),\n",
    "     metrics=['acc', f1])\n",
    "#model.compile(loss='binary_crossentropy',\n",
    "#            optimizer=Adam(lr=1e-4),\n",
    "#            metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_restart = CosineDecayWithRestarts(min_lr = cosine_min , max_lr = cosine_max, lr_decay=lr_decay, steps_per_epoch=len(train_generator),\n",
    "                                        cycle_length=cycle_length, mult_factor=mult_factor)\n",
    "callbacks_list = [checkpoint, cosine_restart]\n",
    "\n",
    "\n",
    "hist2 = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs2, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a4815f3f357736f2664d12e96aa487433c6dda6",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valGen = ProteinDataGenerator(pathsVal,labelsVal, BATCH_SIZE, SHAPE)\n",
    "#customObjects = {}        #Needed for loading models with saved loss functions ... have not tested for no custom objects\n",
    "#customObjects={'f1': f1, 'focal_loss': focal_loss}\n",
    "#model = load_model(modelFile,custom_objects=customObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "\n",
    "def getOptimalT(mdl, valGen):\n",
    "    \n",
    "    valPred = np.empty((0, 28))\n",
    "    valLabels = np.empty((0, 28))\n",
    "    for i in tqdm(range(len(valGen))): \n",
    "        im, lbl = valGen[i]\n",
    "        scores = mdl.predict(im)\n",
    "        valPred = np.append(valPred, scores, axis=0)\n",
    "        valLabels = np.append(valLabels, lbl, axis=0)\n",
    "    print(valPred.shape, valLabels.shape)\n",
    "    \n",
    "    rng = np.arange(0, 1, 0.001)\n",
    "    f1s = np.zeros((rng.shape[0], 28))\n",
    "    for j,t in enumerate(tqdm(rng)):\n",
    "        for i in range(28):\n",
    "            p = np.array(valPred[:,i]>t, dtype=np.int8)\n",
    "            #scoref1 = K.eval(f1(valLabels[:,i], p))\n",
    "            scoref1 = off1(valLabels[:,i], p, average='binary')\n",
    "            f1s[j,i] = scoref1\n",
    "            \n",
    "    print(np.max(f1s, axis=0))\n",
    "    print(np.mean(np.max(f1s, axis=0)))\n",
    "    \n",
    "    plt.plot(rng, f1s)\n",
    "    T = np.empty(28)\n",
    "    for i in range(28):\n",
    "        T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "    print('Choosing threshold: ', T)\n",
    "#    print('Validation F1-score: ', max(f1s))\n",
    "#    print(T)\n",
    "    \n",
    "    return T, np.mean(np.max(f1s, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valGen = ProteinDataGenerator(pathsVal,labelsVal, BATCH_SIZE, SHAPE)\n",
    "#customObjects = {}        #Needed for loading models with saved loss functions ... have not tested for no custom objects\n",
    "#customObjects={'f1': f1, 'focal_loss': focal_loss}\n",
    "#model = load_model(modelFile,custom_objects=customObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Last model after fine-tuning')\n",
    "T1, ff1 = getOptimalT(model, valGen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as offl\n",
    "\n",
    "def calcF1Score(mdl, valGen, T):\n",
    "    \n",
    "    valPred = np.empty((0, 28))\n",
    "    valLabels = np.empty((0, 28))\n",
    "    preds = np.empty((0, 28))\n",
    "    for i in tqdm(range(len(valGen))): \n",
    "        im, lbl = valGen[i]\n",
    "        scores = mdl.predict(im)\n",
    "        valPred = np.append(valPred, scores, axis=0)\n",
    "        valLabels = np.append(valLabels, lbl, axis=0)\n",
    "    p = np.array(valPred>T, dtype=np.int8)\n",
    "#    scoref1 = K.eval(f1(valLabels, p))\n",
    "    scoref1 = off1(valLabels, p, average='macro')\n",
    "#            f1s[j,i] = scoref1\n",
    "#    preds = lastFullValPred > T\n",
    "    return scoref1\n",
    "#    skf1 = offl(lastFullValPred, lastFullValLabels)\n",
    "#    localf1 = f1(lastFullValPred, lastFullValLabels)\n",
    "    \n",
    "    \n",
    "#    return skf1, localf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#K.clear_session()\n",
    "#valGen = ProteinDataGenerator(pathsVal,labelsVal, BATCH_SIZE, SHAPE)\n",
    "#bestModelPlateau = load_model('../working/ResNet50A.h5', custom_objects={'f1': f1}) #, 'f1_loss': f1_loss})\n",
    "T = T1\n",
    "skf1 = calcF1Score(model, valGen, T)\n",
    "print(\"F1 score of model\", skf1, '   using ',T)\n",
    "T = threshold\n",
    "skf2 = calcF1Score(model, valGen, T)\n",
    "print(\"F1 score of model\", skf2, '   using ',T)\n",
    "if (skf1 > skf2):\n",
    "    saveT = T1\n",
    "    saveF1 = skf1\n",
    "else:\n",
    "    saveT = threshold\n",
    "    saveF1 = skf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = T1\n",
    "# Create submit\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "#model.load_weights(modelFile)\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../input/test/', name)\n",
    "    image = train_generator.load_image(path)\n",
    "    score_predict = model.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=T]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "np.save('draw_predict_ResNet50.npy', score_predict)\n",
    "submit.to_csv(submitFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as offl\n",
    "\n",
    "def getPredLabels(mdl, generator, T):\n",
    "    \n",
    "    pred = np.empty((0, 28))\n",
    "    labels = np.empty((0, 28))\n",
    "    for i in tqdm(range(len(generator))): \n",
    "        im, lbl = generator[i]\n",
    "        scores = mdl.predict(im)\n",
    "        pred = np.append(pred, scores, axis=0)\n",
    "        labels = np.append(labels, lbl, axis=0)\n",
    "\n",
    "    p = np.array(pred>T, dtype=np.float)\n",
    "    return p, labels, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = T1\n",
    "p,labels,predProbs = getPredLabels(model, valGen, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "one_hot = labels.sum() / (labels.shape[0]*labels.shape[1]) * 100\n",
    "zero_hot = ((labels.shape[0]*labels.shape[1] - labels.sum()) / (labels.shape[0]*labels.shape[1])) * 100\n",
    "print(one_hot, zero_hot, labels.sum(), labels.shape[0], labels.shape[1], labels.shape[0]*labels.shape[1])\n",
    "fpredProbs = predProbs.reshape(-1)\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "sns.distplot(fpredProbs * 100, color=\"DodgerBlue\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Probability in %\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].set_title(\"Predicted probabilities\")\n",
    "sns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\n",
    "ax[1].set_ylim([0,100])\n",
    "ax[1].set_title(\"True target label count\")\n",
    "ax[1].set_ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "eps = np.finfo(float).eps\n",
    "def ml_confusion_matrix_counts(yt, yp, classes):\n",
    "    instcount = yt.shape[0]\n",
    "    n_classes = classes.shape[0]\n",
    "    mt = np.zeros((n_classes, 4))\n",
    "    mtx = np.zeros((n_classes, 7))\n",
    "    for i in range(instcount):\n",
    "        for c in range(n_classes):\n",
    "            mt[c,0] += 1 if yt[i,c]==1 and yp[i,c]==1 else 0  #TP\n",
    "            mt[c,1] += 1 if yt[i,c]==1 and yp[i,c]==0 else 0  #FN\n",
    "            mt[c,2] += 1 if yt[i,c]==0 and yp[i,c]==0 else 0  #TN\n",
    "            mt[c,3] += 1 if yt[i,c]==0 and yp[i,c]==1 else 0  #FP\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "eps = np.finfo(float).eps\n",
    "def ml_confusion_matrix(yt, yp, classes):\n",
    "    instcount = yt.shape[0]\n",
    "    n_classes = classes.shape[0]\n",
    "    mt = np.zeros((n_classes, 4))\n",
    "    mtx = np.zeros((n_classes, 7))\n",
    "    for i in range(instcount):\n",
    "        for c in range(n_classes):\n",
    "            mt[c,0] += 1 if yt[i,c]==1 and yp[i,c]==1 else 0  #TP\n",
    "            mt[c,1] += 1 if yt[i,c]==1 and yp[i,c]==0 else 0  #FN\n",
    "            mt[c,2] += 1 if yt[i,c]==0 and yp[i,c]==0 else 0  #TN\n",
    "            mt[c,3] += 1 if yt[i,c]==0 and yp[i,c]==1 else 0  #FP\n",
    "    for c in range(n_classes):\n",
    "        mtx[c,0] = mt[c,0]/(mt[c,0] + mt[c,3]) if (mt[c,0] + mt[c,3]) > 0 else 0\n",
    "        mtx[c,1] = mt[c,3]/(mt[c,0] + mt[c,3]) if (mt[c,0] + mt[c,3]) > 0 else 0\n",
    "        mtx[c,2] = mt[c,1]/(mt[c,1] + mt[c,2]) if (mt[c,1] + mt[c,2]) > 0 else 0\n",
    "        mtx[c,3] = mt[c,2]/(mt[c,1] + mt[c,2]) if (mt[c,1] + mt[c,2]) > 0 else 0\n",
    "        mtx[c,4] = mt[c,0]/(mt[c,0] + mt[c,3]) if (mt[c,0] + mt[c,3]) > 0 else 0\n",
    "        mtx[c,5] = mt[c,0]/(mt[c,0] + mt[c,1]) if (mt[c,0] + mt[c,1]) > 0 else 0\n",
    "        mtx[c,6] = 2 * (mtx[c,4] * mtx[c,5])/(mtx[c,4] + mtx[c,5]) if (mtx[c,4] + mtx[c,5]) > 0 else 0\n",
    "    return mtx\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clss = np.array(list(label_names.values()))\n",
    "n_classes = len(clss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx = ml_confusion_matrix(labels, p, clss)\n",
    "mt = ml_confusion_matrix_counts(labels, p, clss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,15),facecolor='w', edgecolor='k')\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.yticks(tick_marks, clss)\n",
    "\n",
    "ax[1].set_title(\"TPr  FPr  TNr   FNr   PR   RC    F1\")\n",
    "ax[1].imshow(mtx, interpolation='nearest',cmap='Reds')\n",
    "ax[1].set_title(\"TPr  FPr  TNr   FNr   PR   RC    F1\")\n",
    "for i, j in itertools.product(range(n_classes), range(7)):\n",
    "    ax[1].text(j, i, round(mtx[i][j],2), horizontalalignment=\"center\")\n",
    "\n",
    "ax[0].imshow(mt, interpolation='nearest',cmap='Reds')\n",
    "ax[0].set_title(\"TP  FN  TN   FP   \")\n",
    "for i, j in itertools.product(range(n_classes), range(4)):\n",
    "    ax[0].text(j, i, int(mt[i][j]), horizontalalignment=\"center\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtx = ml_confusion_matrix(labels, p, clss)\n",
    "\n",
    "#plt.figure(num=None, figsize=(5, 15), dpi=100, facecolor='w', edgecolor='k')\n",
    "#plt.imshow(mtx, interpolation='nearest',cmap='Reds')\n",
    "#plt.title(\"TPr  FPr  TNr   FNr   PR   RC    F1\")\n",
    "#tick_marks = np.arange(n_classes)\n",
    "#plt.yticks(tick_marks, clss)\n",
    "#for i, j in itertools.product(range(n_classes), range(7)):\n",
    "#    plt.text(j, i, round(mtx[i][j],2), horizontalalignment=\"center\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "#plt.ylabel('labels')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mt = ml_confusion_matrix_counts(labels, p, clss)\n",
    "#plt.figure(num=None, figsize=(8, 15), dpi=100, facecolor='w', edgecolor='k')\n",
    "#plt.imshow(mt, interpolation='nearest',cmap='Reds')\n",
    "#plt.title(\"TP  FN  TN   FP   \")\n",
    "#tick_marks = np.arange(n_classes)\n",
    "#plt.yticks(tick_marks, clss)\n",
    "#for i, j in itertools.product(range(n_classes), range(4)):\n",
    "#    plt.text(j, i, int(mt[i][j]), horizontalalignment=\"center\")\n",
    "\n",
    "    #plt.tight_layout()\n",
    "#plt.ylabel('counts')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "def load_image(basepath, image_id):\n",
    "    images = np.zeros(shape=(512,512,4))\n",
    "    images[:,:,0] = imread(image_id + \"_green\" + \".png\")\n",
    "    images[:,:,1] = imread(image_id + \"_red\" + \".png\")\n",
    "    images[:,:,2] = imread(image_id + \"_blue\" + \".png\")\n",
    "    images[:,:,3] = imread(image_id + \"_yellow\" + \".png\")\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "def make_image_row(image, subax, title):\n",
    "    subax[0].imshow(image[:,:,0], cmap=\"Greens\")\n",
    "    subax[0].set_title(title)\n",
    "    subax[1].imshow(image[:,:,1], cmap=\"Reds\")\n",
    "    subax[1].set_title(\"stained microtubules\")\n",
    "    subax[2].imshow(image[:,:,2], cmap=\"Blues\")\n",
    "    subax[2].set_title(\"stained nucleus\")\n",
    "    if (image.shape[2] == 4):\n",
    "        subax[3].imshow(image[:,:,3], cmap=\"Oranges\")\n",
    "        subax[3].set_title(\"stained endoplasmatic reticulum\")\n",
    "    return subax\n",
    "def make_title(lab):\n",
    "    file_targets = [label_names[idx] for idx in range(28) if int(lab[idx]) ==1]\n",
    "#    file_targets = np.where(lab[file_id] == 1)[0]\n",
    "    title = \" - \"\n",
    "    for n in file_targets:\n",
    "        title += n + \" - \"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetGroupIterator:\n",
    "    \n",
    "    def __init__(self, target_names, batch_size, basepath):\n",
    "        self.target_names = target_names\n",
    "        self.target_list = [reverse_train_labels[key] for key in target_names]\n",
    "        self.batch_shape = (batch_size, 512, 512,4)\n",
    "        self.basepath = basepath\n",
    "        self.images_identifier = []\n",
    "    \n",
    "    def find_matching_data_entries(self):\n",
    "        y = np.zeros(28)\n",
    "        cnt = 0\n",
    "        for key in self.target_list:\n",
    "            y[int(key)] = 1\n",
    "            cnt += 1\n",
    "        for i in range(len(labelsTrain)):\n",
    "            if ((int(np.dot(labelsTrain[i], y)) == cnt)):\n",
    "                self.images_identifier.append(i)\n",
    "        \n",
    "#        self.images_identifier = train_labels[train_labels.check_col==1].Id.values\n",
    "#        train_labels.drop(\"check_col\", axis=1, inplace=True)\n",
    "    \n",
    "    def check_subset(self, targets):\n",
    "        return np.where(set(targets).issubset(set(self.target_list)), 1, 0)\n",
    " #        return np.where(set(targets).issubset(set(self.target_list)), 1, 0)\n",
    "    \n",
    "    def get_loader(self):\n",
    "        filenames = []\n",
    "        idx = 0\n",
    "        images = np.zeros(self.batch_shape)\n",
    "        for image_id in self.images_identifier:\n",
    "            images[idx,:,:,:] = load_image(self.basepath, pathsTrain[image_id])\n",
    "            filenames.append(image_id)\n",
    "            idx += 1\n",
    "            if idx == self.batch_shape[0]:\n",
    "                yield filenames, images\n",
    "                filenames = []\n",
    "                images = np.zeros(self.batch_shape)\n",
    "                idx = 0\n",
    "        if idx > 0:\n",
    "            yield filenames, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFailedPredImagesLabels(mdl, generator,numFailures, T):\n",
    "    images = []\n",
    "    labels = []\n",
    "    ps = []\n",
    "    cnt = 0\n",
    "    j = 0\n",
    "    while (cnt < numFailures):\n",
    "        im, lbl = generator[j]\n",
    "        scores = mdl.predict(im)\n",
    "        p = np.array(scores>T, dtype=np.float)\n",
    "        i = 0\n",
    "        while i < len(lbl) and cnt < numFailures:\n",
    "            if (np.dot(p[i], lbl[i]) == 0):\n",
    "                images.append(im[i])\n",
    "                ps.append(p[i])\n",
    "                labels.append(lbl[i])\n",
    "                cnt += 1\n",
    "            i += 1\n",
    "        j += 1\n",
    "    return np.array(images),np.array(ps), np.array(labels)\n",
    "\n",
    "def getNames(y):\n",
    "    names = []\n",
    "    for id in range(y.shape[0]):\n",
    "        idNames = [label_names[idx] for idx in range(28) if y[id][idx] ==1]\n",
    "        for aName in idNames:\n",
    "            if aName not in names:\n",
    "                names.append(aName)\n",
    "    return list(names)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFailures = 3\n",
    "T = threshold\n",
    "images, ps, trueLabels = getFailedPredImagesLabels(model, valGen,numFailures, T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numChannels = images.shape[0]\n",
    "fig, ax = plt.subplots(numFailures,numChannels,figsize=(20,5*numFailures))\n",
    "fig.text(.5, .05, \"Failed Classification Images\", ha='center')\n",
    "if ax.shape == (4,):\n",
    "    ax = ax.reshape(1,-1)\n",
    "for n in range(numFailures):\n",
    "    make_image_row(images[n], ax[n], make_title(ps[n]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numImages = 3\n",
    "numChannels =4\n",
    "for i in range(numFailures):\n",
    "    proteinNames = [label_names[idx] for idx in range(28) if ps[i][idx] ==1]\n",
    "    imageloader = TargetGroupIterator(proteinNames,numImages , '../input/train')\n",
    "    imageloader.find_matching_data_entries()\n",
    "    iterator = imageloader.get_loader()\n",
    "    file_ids, images = next(iterator)\n",
    "    fig, ax = plt.subplots(numImages,numChannels,figsize=(20,5*numImages))\n",
    "    txt = \"Images for the Predicated Targets from Training Set\"\n",
    "    fig.text(.5, .05, txt, ha='center')\n",
    "    if ax.shape == (4,):\n",
    "        ax = ax.reshape(1,-1)\n",
    "    for n in range(len(file_ids)):\n",
    "        make_image_row(images[n], ax[n], make_title(ps[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numImages = 3\n",
    "numChannels =4\n",
    "for i in range(numFailures):\n",
    "    proteinNames = [label_names[idx] for idx in range(28) if labels[i][idx] ==1]\n",
    "    imageloader = TargetGroupIterator(proteinNames,numImages , '../input/train')\n",
    "    imageloader.find_matching_data_entries()\n",
    "    iterator = imageloader.get_loader()\n",
    "    file_ids, images = next(iterator)\n",
    "    fig, ax = plt.subplots(numImages,numChannels,figsize=(20,5*numImages))\n",
    "    txt = \"Images for the True Targets from Training Set \"\n",
    "    fig.text(.5, .05, txt, ha='center')\n",
    "    if ax.shape == (4,):\n",
    "        ax = ax.reshape(1,-1)\n",
    "    for n in range(len(file_ids)):\n",
    "        make_image_row(images[n], ax[n], make_title(labels[n]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better formatting needed hours, minutes, etc\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(int(elapsed/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics\")\n",
    "print(\"F1 \", saveF1, \"T \", saveT, \"lr1 \",lr1, \"lr2 \",lr2, \"epochs1 \", epochs1, \"epochs2 \", epochs2, \"BATCH_SIZE \", BATCH_SIZE, \"SHAPE \", SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
