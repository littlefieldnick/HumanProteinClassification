{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from scipy.misc import imread\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "modelFile = '../working/ConvPool_A.h5'\n",
    "submitFile = '../working/ConvPool_A.csv'\n",
    "loss_function = 'binary_crossentropy'\n",
    "#loss_function = focal_loss\n",
    "epochs = 10\n",
    "lr = 1e-03 \n",
    "threshold = 0.35 # due to different cost of True Positive vs False Positive, this is the probability threshold to predict the class as 'yes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf; \n",
    "print(tf.__version__)\n",
    "import keras; \n",
    "from keras import backend as K\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cc5f5c13a64028cac0a9b5c24e8d31dffa6ffe0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dd4627557204489522f1e5724053f06adbb2cda"
   },
   "outputs": [],
   "source": [
    "#SIZE = 299   #Inception\n",
    "SIZE = 224   #ResNet50\n",
    "BATCH_SIZE = 64\n",
    "SEED = 777\n",
    "\n",
    "SHAPE = (224, 224, 4)\n",
    "CUST_SHAPE = (128,128,4)\n",
    "#SHAPE = (229, 229, 3)\n",
    "VAL_RATIO = 0.2 # 20 % as validation\n",
    "\n",
    "DIR = '../input'\n",
    "gamma = 2.0\n",
    "import keras\n",
    "epsilon = K.epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55348be2a0a9285c52126c6096a34988e690023a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"DateTime \" + time.strftime(\"%c\"))\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28935778ab602658c7cc429ddfcc3b95d28033b9"
   },
   "outputs": [],
   "source": [
    "label_names = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "\n",
    "reverse_train_labels = dict((v,k) for k,v in label_names.items())\n",
    "\n",
    "def getTargetNames(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    names = []\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        names.append(name)\n",
    "    return row\n",
    "\n",
    "def fill_targets(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        row.loc[name] = 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e5a60df16f7ae3981b4e56b52a4f149237b406f"
   },
   "outputs": [],
   "source": [
    "def getTrainDataset():\n",
    "    \n",
    "    path_to_train = DIR + '/train/'\n",
    "    data = pd.read_csv(DIR + '/train.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n",
    "        y = np.zeros(28)\n",
    "        for key in lbl:\n",
    "            y[int(key)] = 1\n",
    "        paths.append(os.path.join(path_to_train, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n",
    "\n",
    "def getTestDataset():\n",
    "    \n",
    "    path_to_test = DIR + '/test/'\n",
    "    data = pd.read_csv(DIR + '/sample_submission.csv')\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for name in data['Id']:\n",
    "        y = np.ones(28)\n",
    "        paths.append(os.path.join(path_to_test, name))\n",
    "        labels.append(y)\n",
    "\n",
    "    return np.array(paths), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4746cb9f9e4ee666cc3f579fed84d31a4c651148"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProteinDataGenerator(keras.utils.Sequence):\n",
    "            \n",
    "    def __init__(self, paths, labels, batch_size, shape, shuffle = False, use_cache = False, augment = False,channels=3):\n",
    "        self.paths, self.labels = paths, labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shape = shape\n",
    "        self.shuffle = shuffle\n",
    "        self.use_cache = use_cache\n",
    "        self.augment = augment\n",
    "        self.channels = channels\n",
    "        if use_cache == True:\n",
    "            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], shape[2]), dtype=np.float16)\n",
    "            self.is_cached = np.zeros((paths.shape[0]))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        paths = np.array([self.paths[k] for k in indexes])\n",
    "        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n",
    "        # Generate data\n",
    "        if self.use_cache == True:\n",
    "            X = self.cache[indexes]\n",
    "            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n",
    "                image = self.load_image(path)\n",
    "                self.is_cached[indexes[i]] = 1\n",
    "                self.cache[indexes[i]] = image\n",
    "                X[i] = image\n",
    "        else:\n",
    "            for i, path in enumerate(paths):\n",
    "                X[i] = self.load_image(path)\n",
    "\n",
    "        y = np.array([self.labels[k] for k in indexes])\n",
    "#        y = self.labels[indexes]\n",
    "                \n",
    "        if self.augment == True:\n",
    "            seq = iaa.Sequential([\n",
    "                iaa.OneOf([\n",
    "                    iaa.Fliplr(0.5), # horizontal flips\n",
    "                    iaa.Flipud(0.5),  #vertical flips\n",
    "                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "                    # But we only blur about 50% of all images.  **** Not clear that this is doing anything ****\n",
    "                    iaa.Sometimes(0.5,\n",
    "                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                    ),\n",
    "                    # Strengthen or weaken the contrast in each image.\n",
    "#                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                    # Strengthen Contrast !\n",
    "                    iaa.ContrastNormalization((0.1, 0.25)),\n",
    "                    # Add gaussian noise.\n",
    "                    # For 50% of all images, we sample the noise once per pixel.\n",
    "                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "                    # channel. This can change the color (not only brightness) of the\n",
    "                    # pixels.\n",
    " #                   iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                    # Make some images brighter and some darker.\n",
    "                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "                    # which can end up changing the color of the images.\n",
    "                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                    # Apply affine transformations to each image.\n",
    "                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "                    iaa.Affine(rotate=90),\n",
    "                    iaa.Affine(rotate=180),\n",
    "                    iaa.Affine(rotate=270),\n",
    "                    iaa.Affine(\n",
    "                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "                        rotate=(-180, 180),\n",
    "                        shear=(-8, 8)\n",
    "                    )\n",
    "                ])], random_order=True)\n",
    "\n",
    "            X = seq.augment_images(X)\n",
    "#        if self.augment == True:\n",
    "#            seq = iaa.Sequential([\n",
    "#                iaa.OneOf([\n",
    "#                    iaa.Fliplr(0.5), # horizontal flips\n",
    "#                    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "#                    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "#                    # But we only blur about 50% of all images.\n",
    "#                    iaa.Sometimes(0.5,\n",
    "#                        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "#                    ),\n",
    "#                    # Strengthen or weaken the contrast in each image.\n",
    "#                    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "#                    # Add gaussian noise.\n",
    "#                    # For 50% of all images, we sample the noise once per pixel.\n",
    "#                    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "#                    # channel. This can change the color (not only brightness) of the\n",
    "#                    # pixels.\n",
    "#                   iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "#                    # Make some images brighter and some darker.\n",
    "#                    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "#                    # which can end up changing the color of the images.\n",
    "#                    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "#                    # Apply affine transformations to each image.\n",
    "#                    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "#                    iaa.Affine(\n",
    "#                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "#                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "#                        rotate=(-180, 180),\n",
    "#                        shear=(-8, 8)\n",
    "#                    )\n",
    "#                ])], random_order=True)\n",
    "#\n",
    "#            X = np.concatenate((X, seq.augment_images(X), seq.augment_images(X), seq.augment_images(X)), 0)\n",
    "#            y = np.concatenate((y, y, y, y), 0)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        # Updates indexes after each epoch\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
    "        for item in (self[i] for i in range(len(self))):\n",
    "            yield item\n",
    "    \n",
    "    def load4Channel(self, path):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "        np.array(image_red_ch), \n",
    "        np.array(image_yellow_ch),     \n",
    "        np.array(image_green_ch), \n",
    "        np.array(image_blue_ch)), -1)\n",
    "        image = cv2.resize(image, (self.shape[0], self.shape[1]))\n",
    "        im = np.divide(image, 255) \n",
    "        return im\n",
    "\n",
    "    def load3Channel(self, path):\n",
    "        image_red_ch = Image.open(path+'_red.png')\n",
    "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
    "        image_green_ch = Image.open(path+'_green.png')\n",
    "        image_blue_ch = Image.open(path+'_blue.png')\n",
    "        image = np.stack((\n",
    "        np.array(image_red_ch), \n",
    "        np.array(image_green_ch), \n",
    "        np.array(image_blue_ch)), -1)\n",
    "        image = cv2.resize(image, (self.shape[0], self.shape[1]))\n",
    "        im = np.divide(image, 255)\n",
    "        return im\n",
    "\n",
    "    def load_image(self, path):\n",
    "        if self.channels == 3:\n",
    "            return self.load3Channel(path)\n",
    "        else:\n",
    "            return self.load4Channel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e517caa0946e5213d8e309d6227f14235cda41fc"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Multiply, Input, GlobalAveragePooling2D, Flatten, Conv2D, Concatenate, ReLU, LeakyReLU, BatchNormalization, MaxPooling2D, Lambda\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam  \n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def conv_pool_cnn(model_input, n_out):\n",
    "    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same')(model_input)\n",
    "    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)\n",
    "    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)\n",
    "    x = Conv2D(192, (1, 1), activation='relu')(x)\n",
    "    x = Conv2D(28, (1, 1))(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Activation(activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(model_input, x, name='conv_pool_cnn')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cdedb830e204bd144816c235c42dee8299fa6d9"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "checkpoint = ModelCheckpoint(modelFile, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "#checkpoint = ModelCheckpoint('../working/ResNet50A.h5', monitor='val_loss', verbose=1, \n",
    "#                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6)\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f92af84c5fc44675400fe585794f6b7e59bc2a4"
   },
   "outputs": [],
   "source": [
    "# split data into train, valid\n",
    "paths, labels = getTrainDataset()\n",
    "\n",
    "# divide to \n",
    "keys = np.arange(paths.shape[0], dtype=np.int)  \n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(keys)\n",
    "lastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n",
    "\n",
    "#print(lastIndex, lastTrainIndex, int(lastTrainIndex * 0.8))\n",
    "pathsTrain = paths[0:lastTrainIndex]\n",
    "labelsTrain = labels[0:lastTrainIndex]\n",
    "pathsVal = paths[lastTrainIndex:]\n",
    "labelsVal = labels[lastTrainIndex:]\n",
    "\n",
    "print(paths.shape, labels.shape)\n",
    "print(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n",
    "\n",
    "train_generator = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, CUST_SHAPE, use_cache=False, channels=4, augment = True, shuffle = False)\n",
    "validation_generator = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, CUST_SHAPE, use_cache=False, channels=4, augment=True, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60edee8bc4ffec1c3e9c858cef6e080117e2372b"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "protein_model_input = Input(CUST_SHAPE)\n",
    "protein_classes = len(label_names)\n",
    "\n",
    "model = conv_pool_cnn(protein_model_input, protein_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b66e708c80ca97640098e4a06bceb21b4f5e4ef5"
   },
   "outputs": [],
   "source": [
    "def focal_loss(y_true, y_pred):\n",
    "    pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n",
    "    pt = K.clip(pt, epsilon, 1-epsilon)\n",
    "    CE = -K.log(pt)\n",
    "    FL = K.pow(1-pt, gamma) * CE\n",
    "    loss = K.sum(FL, axis=1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1PR(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1), K.mean(p), K.mean(r)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "\n",
    "def fbeta(y_true, y_pred, threshold_shift=0):\n",
    "    beta = 2\n",
    "\n",
    "    # just in case of hipster activation at the final layer\n",
    "    y_pred = K.clip(y_pred, 0, 1)\n",
    "\n",
    "    # shifting the prediction threshold from .5 if needed\n",
    "    y_pred_bin = K.round(y_pred + threshold_shift)\n",
    "\n",
    "    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n",
    "    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    return (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "449baf1f896eec95b1cfa034949b09e7ed8039f7"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=loss_function,\n",
    "    optimizer=Adam(lr),    \n",
    "    metrics=['acc', f1])\n",
    "model.summary()\n",
    "model.load_weights(\"../working/ResNet50BXF.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ff7b2905c2dfeb504a397fb17ad3dd352ba68b7"
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e74281ad7727f21511048aac2b5fe6a2d355d585"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tune Model with Bigger Image\n",
    "train_generator = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, SHAPE, use_cache=False, channels=4, augment = True, shuffle = False)\n",
    "validation_generator = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, SHAPE, use_cache=False, channels=4, augment=True, shuffle = False)\n",
    "\n",
    "# Build new model for larger image, transfer prev model weights over and continue training\n",
    "model2 = conv_pool_cnn(Input(SHAPE), protein_classes)\n",
    "model2.set_weights(model.get_weights())\n",
    "model2.compile(\n",
    "    loss=loss_function,\n",
    "    optimizer=Adam(lr),    \n",
    "    metrics=['acc', f1])\n",
    "model2.summary()\n",
    "\n",
    "hist2 = model2.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs, \n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tune Model with Bigger Image\n",
    "# train_generator = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, (512, 512, 4), use_cache=False, channels=4, augment = True, shuffle = False)\n",
    "# validation_generator = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, (512, 512, 4) use_cache=False, channels=4, augment=True, shuffle = False)\n",
    "\n",
    "# Build new model for larger image, transfer prev model weights over and continue training\n",
    "# Ran out of memory for 3rd stage of training\n",
    "\n",
    "# model3 = conv_pool_cnn(Input((300, 300, 4)), protein_classes)\n",
    "# model3.set_weights(model2.get_weights())\n",
    "# model3.compile(\n",
    "#     loss=loss_function,\n",
    "#     optimizer=Adam(lr),    \n",
    "#     metrics=['acc', f1])\n",
    "# model3.summary()\n",
    "\n",
    "# hist3 = model3.fit_generator(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=len(train_generator),\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=8,\n",
    "#     epochs=epochs, \n",
    "#     verbose=1,\n",
    "#     callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist3.epoch, hist3.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist3.epoch, hist3.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist3.epoch, hist3.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist3.epoch, hist3.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e20a9620513bfff07002403fa19a9661eb952353"
   },
   "outputs": [],
   "source": [
    "valGen = ProteinDataGenerator(pathsVal,labelsVal, BATCH_SIZE, SHAPE, channels=4)\n",
    "#customObjects = {}        #Needed for loading models with saved loss functions ... have not tested for no custom objects\n",
    "#customObjects={'f1': f1, 'focal_loss': focal_loss}\n",
    "#customObjects={'f1': f1}\n",
    "#model = load_model(modelFile,custom_objects=customObjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as off1\n",
    "\n",
    "def getOptimalT(mdl, valGen):\n",
    "    \n",
    "    valPred = np.empty((0, 28))\n",
    "    valLabels = np.empty((0, 28))\n",
    "    for i in tqdm(range(len(valGen))): \n",
    "        im, lbl = valGen[i]\n",
    "        scores = mdl.predict(im)\n",
    "        valPred = np.append(valPred, scores, axis=0)\n",
    "        valLabels = np.append(valLabels, lbl, axis=0)\n",
    "    print(valPred.shape, valLabels.shape)\n",
    "    \n",
    "    rng = np.arange(0, 1, 0.001)\n",
    "    f1s = np.zeros((rng.shape[0], 28))\n",
    "    for j,t in enumerate(tqdm(rng)):\n",
    "        for i in range(28):\n",
    "            p = np.array(valPred[:,i]>t, dtype=np.int8)\n",
    "            #scoref1 = K.eval(f1(valLabels[:,i], p))\n",
    "            scoref1 = off1(valLabels[:,i], p, average='binary')\n",
    "            f1s[j,i] = scoref1\n",
    "            \n",
    "    print(np.max(f1s, axis=0))\n",
    "    print(np.mean(np.max(f1s, axis=0)))\n",
    "    \n",
    "    plt.plot(rng, f1s)\n",
    "    T = np.empty(28)\n",
    "    for i in range(28):\n",
    "        T[i] = rng[np.where(f1s[:,i] == np.max(f1s[:,i]))[0][0]]\n",
    "    print('Choosing threshold: ', T)\n",
    "#    print('Validation F1-score: ', max(f1s))\n",
    "#    print(T)\n",
    "    \n",
    "    return T, np.mean(np.max(f1s, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "169b706a61fd61a62d56d7f742fca2825e16ba0a"
   },
   "outputs": [],
   "source": [
    "print('Last model after fine-tuning')\n",
    "T1, ff1 = getOptimalT(model2, valGen)\n",
    "#T1[T1<0.1] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "556ac1786c9e29f18f840d316d85adda2ea6438a"
   },
   "outputs": [],
   "source": [
    "#OriginalT1 = T1.copy()\n",
    "#T1[T1 < 0.1] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8d41abf83741a6d0b0a6025ca876d3e952d89f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as offl\n",
    "\n",
    "def calcF1Score(mdl, valGen, T):\n",
    "    \n",
    "    valPred = np.empty((0, 28))\n",
    "    valLabels = np.empty((0, 28))\n",
    "    preds = np.empty((0, 28))\n",
    "    for i in tqdm(range(len(valGen))): \n",
    "        im, lbl = valGen[i]\n",
    "        scores = mdl.predict(im)\n",
    "        valPred = np.append(valPred, scores, axis=0)\n",
    "        valLabels = np.append(valLabels, lbl, axis=0)\n",
    "    p = np.array(valPred>T, dtype=np.int8)\n",
    "#    scoref1 = K.eval(f1(valLabels, p))\n",
    "    scoref1 = off1(valLabels, p, average='macro')\n",
    "#            f1s[j,i] = scoref1\n",
    "#    preds = lastFullValPred > T\n",
    "    return scoref1\n",
    "#    skf1 = offl(lastFullValPred, lastFullValLabels)\n",
    "#    localf1 = f1(lastFullValPred, lastFullValLabels)\n",
    "    \n",
    "    \n",
    "#    return skf1, localf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ab06f19a217959a4de76fdb1fde259b40b711e1"
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#K.clear_session()\n",
    "#valGen = ProteinDataGenerator(pathsVal,labelsVal, BATCH_SIZE, SHAPE)\n",
    "#bestModelPlateau = load_model('../working/ResNet50A.h5', custom_objects={'f1': f1}) #, 'f1_loss': f1_loss})\n",
    "T = T1\n",
    "skf1 = calcF1Score(model2, valGen, T)\n",
    "print(\"F1 score of model\", skf1, '   using ',T)\n",
    "T = threshold\n",
    "skf2 = calcF1Score(model2, valGen, T)\n",
    "print(\"F1 score of model\", skf2, '   using ',T)\n",
    "if (skf1 > skf2):\n",
    "    saveT = T1\n",
    "    saveF1 = skf1\n",
    "else:\n",
    "    saveT = threshold\n",
    "    saveF1 = skf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "701b2712d151cb2118c82c631c06a5ad4e2477ec"
   },
   "outputs": [],
   "source": [
    "T = T1\n",
    "# Create submit\n",
    "submit = pd.read_csv('../input/sample_submission.csv')\n",
    "predicted = []\n",
    "draw_predict = []\n",
    "#model.load_weights(modelFile)\n",
    "for name in tqdm(submit['Id']):\n",
    "    path = os.path.join('../input/test/', name)\n",
    "    image = train_generator.load_image(path)\n",
    "    score_predict = model2.predict(image[np.newaxis])[0]\n",
    "    draw_predict.append(score_predict)\n",
    "    label_predict = np.arange(28)[score_predict>=T]\n",
    "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
    "    predicted.append(str_predict_label)\n",
    "\n",
    "submit['Predicted'] = predicted\n",
    "np.save('draw_predict_ResNet50.npy', score_predict)\n",
    "submit.to_csv(submitFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c55d208eb6203b0b4b9373d2f52fc1dd6120c44"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score as offl\n",
    "\n",
    "def getPredLabels(mdl, generator, T):\n",
    "    \n",
    "    pred = np.empty((0, 28))\n",
    "    labels = np.empty((0, 28))\n",
    "    for i in tqdm(range(len(generator))): \n",
    "        im, lbl = generator[i]\n",
    "        scores = mdl.predict(im)\n",
    "        pred = np.append(pred, scores, axis=0)\n",
    "        labels = np.append(labels, lbl, axis=0)\n",
    "\n",
    "    p = np.array(pred>T, dtype=np.float)\n",
    "    return p, labels, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "874c9c2169d8321c8f04c50af98d9af0a853077e"
   },
   "outputs": [],
   "source": [
    "T = T1\n",
    "p,labels,predProbs = getPredLabels(model2, valGen, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6e9b54896da74aacbb322e706b8705c974e684a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "one_hot = labels.sum() / (labels.shape[0]*labels.shape[1]) * 100\n",
    "zero_hot = ((labels.shape[0]*labels.shape[1] - labels.sum()) / (labels.shape[0]*labels.shape[1])) * 100\n",
    "print(one_hot, zero_hot, labels.sum(), labels.shape[0], labels.shape[1], labels.shape[0]*labels.shape[1])\n",
    "fpredProbs = predProbs.reshape(-1)\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "sns.distplot(fpredProbs * 100, color=\"DodgerBlue\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Probability in %\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].set_title(\"Predicted probabilities\")\n",
    "sns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\n",
    "ax[1].set_ylim([0,100])\n",
    "ax[1].set_title(\"True target label count\")\n",
    "ax[1].set_ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2d5cee06bb555ad72459ba5f73920d1a12cfe40"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "eps = np.finfo(float).eps\n",
    "def ml_confusion_matrix_counts(yt, yp, classes):\n",
    "    instcount = yt.shape[0]\n",
    "    n_classes = classes.shape[0]\n",
    "    mt = np.zeros((n_classes, 4))\n",
    "    mtx = np.zeros((n_classes, 7))\n",
    "    for i in range(instcount):\n",
    "        for c in range(n_classes):\n",
    "            mt[c,0] += 1 if yt[i,c]==1 and yp[i,c]==1 else 0  #TP\n",
    "            mt[c,1] += 1 if yt[i,c]==1 and yp[i,c]==0 else 0  #FN\n",
    "            mt[c,2] += 1 if yt[i,c]==0 and yp[i,c]==0 else 0  #TN\n",
    "            mt[c,3] += 1 if yt[i,c]==0 and yp[i,c]==1 else 0  #FP\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b0e5ef9a76a81812f85c018ccc34e6f2d3ae397"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "eps = np.finfo(float).eps\n",
    "def ml_confusion_matrix(yt, yp, classes):\n",
    "    instcount = yt.shape[0]\n",
    "    n_classes = classes.shape[0]\n",
    "    mt = np.zeros((n_classes, 4))\n",
    "    mtx = np.zeros((n_classes, 7))\n",
    "    for i in range(instcount):\n",
    "        for c in range(n_classes):\n",
    "            mt[c,0] += 1 if yt[i,c]==1 and yp[i,c]==1 else 0  #TP\n",
    "            mt[c,1] += 1 if yt[i,c]==1 and yp[i,c]==0 else 0  #FN\n",
    "            mt[c,2] += 1 if yt[i,c]==0 and yp[i,c]==0 else 0  #TN\n",
    "            mt[c,3] += 1 if yt[i,c]==0 and yp[i,c]==1 else 0  #FP\n",
    "    for c in range(n_classes):\n",
    "        mtx[c,0] = mt[c,0]/(mt[c,0] + mt[c,3]) if (mt[c,0] + mt[c,3]) > 0 else 0\n",
    "        mtx[c,1] = mt[c,3]/(mt[c,0] + mt[c,3]) if (mt[c,0] + mt[c,3]) > 0 else 0\n",
    "        mtx[c,2] = mt[c,1]/(mt[c,1] + mt[c,2]) if (mt[c,1] + mt[c,2]) > 0 else 0\n",
    "        mtx[c,3] = mt[c,2]/(mt[c,1] + mt[c,2]) if (mt[c,1] + mt[c,2]) > 0 else 0\n",
    "        mtx[c,4] = mt[c,0]/(mt[c,0] + mt[c,3]) if (mt[c,0] + mt[c,3]) > 0 else 0\n",
    "        mtx[c,5] = mt[c,0]/(mt[c,0] + mt[c,1]) if (mt[c,0] + mt[c,1]) > 0 else 0\n",
    "        mtx[c,6] = 2 * (mtx[c,4] * mtx[c,5])/(mtx[c,4] + mtx[c,5]) if (mtx[c,4] + mtx[c,5]) > 0 else 0\n",
    "    return mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clss = np.array(list(label_names.values()))\n",
    "n_classes = len(clss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].set_title('loss')\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"loss\"], label=\"Train loss\")\n",
    "ax[0].plot(hist2.epoch, hist2.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[1].set_title('acc')\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"f1\"], label=\"Train F1\")\n",
    "ax[1].plot(hist2.epoch, hist2.history[\"val_f1\"], label=\"Validation F1\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx = ml_confusion_matrix(labels, p, clss)\n",
    "mt = ml_confusion_matrix_counts(labels, p, clss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,15),facecolor='w', edgecolor='k')\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.yticks(tick_marks, clss)\n",
    "\n",
    "ax[1].set_title(\"TPr  FPr  TNr   FNr   PR   RC    F1\")\n",
    "ax[1].imshow(mtx, interpolation='nearest',cmap='Reds')\n",
    "ax[1].set_title(\"TPr  FPr  TNr   FNr   PR   RC    F1\")\n",
    "for i, j in itertools.product(range(n_classes), range(7)):\n",
    "    ax[1].text(j, i, round(mtx[i][j],2), horizontalalignment=\"center\")\n",
    "\n",
    "ax[0].imshow(mt, interpolation='nearest',cmap='Reds')\n",
    "ax[0].set_title(\"TP  FN  TN   FP   \")\n",
    "for i, j in itertools.product(range(n_classes), range(4)):\n",
    "    ax[0].text(j, i, int(mt[i][j]), horizontalalignment=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "def load_image(basepath, image_id):\n",
    "    images = np.zeros(shape=(512,512,4))\n",
    "    images[:,:,0] = imread(image_id + \"_green\" + \".png\")\n",
    "    images[:,:,1] = imread(image_id + \"_red\" + \".png\")\n",
    "    images[:,:,2] = imread(image_id + \"_blue\" + \".png\")\n",
    "    images[:,:,3] = imread(image_id + \"_yellow\" + \".png\")\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "def make_image_row(image, subax, title):\n",
    "    subax[0].imshow(image[:,:,0], cmap=\"Greens\")\n",
    "    subax[0].set_title(title)\n",
    "    subax[1].imshow(image[:,:,1], cmap=\"Reds\")\n",
    "    subax[1].set_title(\"stained microtubules\")\n",
    "    subax[2].imshow(image[:,:,2], cmap=\"Blues\")\n",
    "    subax[2].set_title(\"stained nucleus\")\n",
    "    if (image.shape[2] == 4):\n",
    "        subax[3].imshow(image[:,:,3], cmap=\"Oranges\")\n",
    "        subax[3].set_title(\"stained endoplasmatic reticulum\")\n",
    "    return subax\n",
    "def make_title(lab):\n",
    "    file_targets = [label_names[idx] for idx in range(28) if int(lab[idx]) ==1]\n",
    "#    file_targets = np.where(lab[file_id] == 1)[0]\n",
    "    title = \" - \"\n",
    "    for n in file_targets:\n",
    "        title += n + \" - \"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetGroupIterator:\n",
    "    \n",
    "    def __init__(self, target_names, batch_size, basepath):\n",
    "        self.target_names = target_names\n",
    "        self.target_list = [reverse_train_labels[key] for key in target_names]\n",
    "        self.batch_shape = (batch_size, 512, 512,4)\n",
    "        self.basepath = basepath\n",
    "        self.images_identifier = []\n",
    "    \n",
    "    def find_matching_data_entries(self):\n",
    "        y = np.zeros(28)\n",
    "        cnt = 0\n",
    "        for key in self.target_list:\n",
    "            y[int(key)] = 1\n",
    "            cnt += 1\n",
    "        for i in range(len(labelsTrain)):\n",
    "            if ((int(np.dot(labelsTrain[i], y)) == cnt)):\n",
    "                self.images_identifier.append(i)\n",
    "        \n",
    "#        self.images_identifier = train_labels[train_labels.check_col==1].Id.values\n",
    "#        train_labels.drop(\"check_col\", axis=1, inplace=True)\n",
    "    \n",
    "    def check_subset(self, targets):\n",
    "        return np.where(set(targets).issubset(set(self.target_list)), 1, 0)\n",
    " #        return np.where(set(targets).issubset(set(self.target_list)), 1, 0)\n",
    "    \n",
    "    def get_loader(self):\n",
    "        filenames = []\n",
    "        idx = 0\n",
    "        images = np.zeros(self.batch_shape)\n",
    "        for image_id in self.images_identifier:\n",
    "            images[idx,:,:,:] = load_image(self.basepath, pathsTrain[image_id])\n",
    "            filenames.append(image_id)\n",
    "            idx += 1\n",
    "            if idx == self.batch_shape[0]:\n",
    "                yield filenames, images\n",
    "                filenames = []\n",
    "                images = np.zeros(self.batch_shape)\n",
    "                idx = 0\n",
    "        if idx > 0:\n",
    "            yield filenames, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFailedPredImagesLabels(mdl, generator,numFailures, T):\n",
    "    images = []\n",
    "    labels = []\n",
    "    ps = []\n",
    "    cnt = 0\n",
    "    j = 0\n",
    "    while (cnt < numFailures):\n",
    "        im, lbl = generator[j]\n",
    "        scores = mdl.predict(im)\n",
    "        p = np.array(scores>T, dtype=np.float)\n",
    "        i = 0\n",
    "        while i < len(lbl) and cnt < numFailures:\n",
    "            if (np.dot(p[i], lbl[i]) == 0):\n",
    "                images.append(im[i])\n",
    "                ps.append(p[i])\n",
    "                labels.append(lbl[i])\n",
    "                cnt += 1\n",
    "            i += 1\n",
    "        j += 1\n",
    "    return np.array(images),np.array(ps), np.array(labels)\n",
    "\n",
    "def getNames(y):\n",
    "    names = []\n",
    "    for id in range(y.shape[0]):\n",
    "        idNames = [label_names[idx] for idx in range(28) if y[id][idx] ==1]\n",
    "        for aName in idNames:\n",
    "            if aName not in names:\n",
    "                names.append(aName)\n",
    "    return list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFailures = 3\n",
    "T = threshold\n",
    "images, ps, trueLabels = getFailedPredImagesLabels(model2, valGen,numFailures, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numImages = 3\n",
    "numChannels =4\n",
    "for i in range(numFailures):\n",
    "    proteinNames = [label_names[idx] for idx in range(28) if ps[i][idx] ==1]\n",
    "    imageloader = TargetGroupIterator(proteinNames,numImages , '../input/train')\n",
    "    imageloader.find_matching_data_entries()\n",
    "    iterator = imageloader.get_loader()\n",
    "    file_ids, images = next(iterator)\n",
    "    fig, ax = plt.subplots(numImages,numChannels,figsize=(20,5*numImages))\n",
    "    txt = \"Images for the Predicated Targets from Training Set\"\n",
    "    fig.text(.5, .05, txt, ha='center')\n",
    "    if ax.shape == (4,):\n",
    "        ax = ax.reshape(1,-1)\n",
    "    for n in range(len(file_ids)):\n",
    "        make_image_row(images[n], ax[n], make_title(ps[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numImages = 3\n",
    "numChannels =4\n",
    "for i in range(numFailures):\n",
    "    proteinNames = [label_names[idx] for idx in range(28) if labels[i][idx] ==1]\n",
    "    imageloader = TargetGroupIterator(proteinNames,numImages , '../input/train')\n",
    "    imageloader.find_matching_data_entries()\n",
    "    iterator = imageloader.get_loader()\n",
    "    file_ids, images = next(iterator)\n",
    "    fig, ax = plt.subplots(numImages,numChannels,figsize=(20,5*numImages))\n",
    "    txt = \"Images for the True Targets from Training Set \"\n",
    "    fig.text(.5, .05, txt, ha='center')\n",
    "    if ax.shape == (4,):\n",
    "        ax = ax.reshape(1,-1)\n",
    "    for n in range(len(file_ids)):\n",
    "        make_image_row(images[n], ax[n], make_title(labels[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better formatting needed hours, minutes, etc\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print(int(elapsed/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics\")\n",
    "print(\"F1 \", saveF1, \"T \", saveT, \"lr \",lr, \"STAGE 1 epochs: \", epochs, \"STAGE 2 epochs:\", epochs, \"BATCH_SIZE \", BATCH_SIZE, \"SHAPE STAGE 1 \", CUST_SHAPE, \"SHAPE STAGE 2\", SHAPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
